Metadata-Version: 2.4
Name: pokemon-red-rl
Version: 0.1.0
Summary: Add your description here
Requires-Python: >=3.13
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: einops>=0.8.1
Requires-Dist: gymnasium>=1.2.2
Requires-Dist: matplotlib>=3.10.7
Requires-Dist: mediapy>=1.2.4
Requires-Dist: numpy>=2.3.5
Requires-Dist: pyboy>=2.6.1
Requires-Dist: scikit-image>=0.25.2
Requires-Dist: stable-baselines3>=2.7.1
Dynamic: license-file

# Train RL agents to play Pokemon Red

Train and visualize reinforcement learning agents that play Pokemon Red, with ready-to-use pretrained models, a fast V2 trainer, and optional live streaming to a shared world map.

## Quick links
- [Watch the demo video](https://youtu.be/DcYLT37ImBY)
- [Multiplayer live training broadcast](https://github.com/pwhiddy/pokerl-map-viz/) · [Live map](https://pwhiddy.github.io/pokerl-map-viz/)
- [Join the Discord](http://discord.gg/RvadteZk4G)
- [Windows setup guide](windows-setup-guide.md)
- AMD GPUs: [install PyTorch with ROCm](https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/wsl/howto_wsl.html)

## Prerequisites
- Python 3.10+ (recommended) and `ffmpeg` available on the command line.
- A legally obtained Pokemon Red ROM placed in the repo root as `PokemonRed.gb` (1 MB). Verify with `shasum PokemonRed.gb` → `ea9bcae617fdf159b045185467ae58b2e4a48b9a`.

## Run the pretrained model (V2 recommended)
V2 is preferred; the original flow is identical but uses the `baselines/` directory instead of `v2/`.

1. `cd v2`
2. Install dependencies  
   - Linux: `pip install -r requirements.txt`  
   - macOS: `pip install -r macos_requirements.txt` (SDL might need separate install)
3. Launch the interactive agent:  
   `python run_pretrained_interactive.py`

Controls: arrow keys + `a` / `s` for A/B buttons. Toggle the agent by editing `agent_enabled.txt`. The ROM must stay in the repo root and your working directory must match the chosen version folder.

## Train your own agent (V2)
V2 trains faster, uses less memory, reaches Cerulean, and streams to the global map by default.

1. Follow the setup above in `v2/`.
2. Start training:  
   `python baseline_fast_v2.py`

## Track progress and broadcast
- **Global map broadcast:** Wrap your environment with the [Broadcast Wrapper](baselines/stream_agent_wrapper.py) (V2 has its own `stream_agent_wrapper.py`) to stream runs to the shared map.
- **Local metrics:** Each session renders frames to its directory. View TensorBoard by running `tensorboard --logdir .` inside the session folder and opening `localhost:6006`. Enable Weights & Biases by setting `use_wandb_logging = True` in the training script.

## Static visualization
Map rendering utilities live in `visualization/`.

## Follow-up work
- [Pokemon Red via Reinforcement Learning](https://arxiv.org/abs/2502.19920)
```
@misc{pleines2025pokemon,
  title={Pokemon Red via Reinforcement Learning},
  author={Marco Pleines and Daniel Addis and David Rubinstein and Frank Zimmer and Mike Preuss and Peter Whidden},
  year={2025},
  eprint={2502.19920},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
```
- [Pokemon RL Edition](https://drubinstein.github.io/pokerl/)
- [PokeGym](https://github.com/PufferAI/pokegym)

## Supporting libraries
- [PyBoy](https://github.com/Baekalfen/PyBoy)
- [Stable Baselines 3](https://github.com/DLR-RM/stable-baselines3)
